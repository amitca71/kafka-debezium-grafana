{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bce27ad-2ef9-42c1-82a9-15ced3f15f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.databricks#dbutils-api_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-50b67d9b-a630-407e-a471-77378a516813;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#dbutils-api_2.12;0.0.5 in central\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.819 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/dbutils-api_2.12/0.0.5/dbutils-api_2.12-0.0.5.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#dbutils-api_2.12;0.0.5!dbutils-api_2.12.jar (136ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.0/delta-core_2.12-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.0!delta-core_2.12.jar (1875ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.2/spark-sql-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2!spark-sql-kafka-0-10_2.12.jar (547ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.819/aws-java-sdk-bundle-1.11.819.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.819!aws-java-sdk-bundle.jar (340365ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.0!hadoop-aws.jar (2006ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (4739ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (832ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (478ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (592ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (110ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (225ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (27124ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.2/spark-token-provider-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2!spark-token-provider-kafka-0-10_2.12.jar (241ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (5247ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (346ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (170ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (9717ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (832ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (2486ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (143ms)\n",
      ":: resolution report :: resolve 27059ms :: artifacts dl 398234ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.819 from central in [default]\n",
      "\tcom.databricks#dbutils-api_2.12;0.0.5 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.819] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   20  |   20  |   1   ||   20  |   20  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-50b67d9b-a630-407e-a471-77378a516813\n",
      "\tconfs: [default]\n",
      "\t20 artifacts copied, 0 already retrieved (198147kB/241ms)\n",
      "21/10/21 16:20:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Column, DataFrame, SparkSession, functions\n",
    "from pyspark.sql.functions import *\n",
    "from py4j.java_collections import MapConverter\n",
    "import shutil\n",
    "import random\n",
    "import threading\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster(\"spark://spark:7077\") \n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", 'http://s3:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'minio') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", 'minio123') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020fedbf-0549-468e-a008-01bf9ee80f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting koalas==1.8.1\n",
      "  Downloading koalas-1.8.1-py3-none-any.whl (1.4 MB)\n",
      "     |████████████████████████████████| 1.4 MB 955 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23.2 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (1.3.4)\n",
      "Requirement already satisfied: pyarrow>=0.10 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23.2->koalas==1.8.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23.2->koalas==1.8.1) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->koalas==1.8.1) (1.16.0)\n",
      "Installing collected packages: koalas\n",
      "Successfully installed koalas-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install koalas==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2bf6254-be9f-4876-a02c-70f6e727db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc).builder.appName(\"streaming\").getOrCreate()\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd37b3d-4e15-4e5d-aabf-31c66c57fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/21 16:20:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---+--------------------+-----------+-------------+---------+\n",
      "|               after|before| op|              source|transaction|        ts_ms|partition|\n",
      "+--------------------+------+---+--------------------+-----------+-------------+---------+\n",
      "|{sally.thomas@acm...|  null|  r|{mysql, inventory...|       null|1634832232389|        0|\n",
      "|{annek@noanswer.o...|  null|  r|{mysql, inventory...|       null|1634832232389|        0|\n",
      "|{gbailey@foobar.c...|  null|  r|{mysql, inventory...|       null|1634832232389|        0|\n",
      "|{ed@walker.com, E...|  null|  r|{mysql, inventory...|       null|1634832232389|        0|\n",
      "+--------------------+------+---+--------------------+-----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "OBJECTURL_TEST = 's3a://minio-sink-bucket/topics/customers'\n",
    "\n",
    "# Read the data back from MinIO\n",
    "gnames_df = spark.read.format('json') \\\n",
    "    .load(OBJECTURL_TEST)\n",
    "gnames_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9706eb5e-84d2-4424-a1a6-707b51f2f0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1001, first_name='Sally', last_name='Thomas', email='sally.thomas@acme.com'),\n",
       " Row(id=1004, first_name='Anne', last_name='Kretchmar', email='annek@noanswer.org'),\n",
       " Row(id=1002, first_name='George', last_name='Bailey', email='gbailey@foobar.com'),\n",
       " Row(id=1003, first_name='Edward', last_name='Walker', email='ed@walker.com')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "gnames_df.select(col(\"after.id\"), col(\"after.first_name\"), col(\"after.last_name\"), col(\"after.email\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61c44f8b-a909-4c84-969c-bd945e9ef9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f85b3b68eb0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#getting the schema from existing file\n",
    "schema=spark.read.format('json').load(OBJECTURL_TEST).schema\n",
    "spark.readStream.format(\"json\").schema(schema).load(OBJECTURL_TEST).writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", 's3a://minio-sink-bucket/delta/customer/_checkpoints/streaming-agg').start('s3a://minio-sink-bucket/bronze/customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0cbfd2a-0e29-493a-bb1f-ce4d805b6968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[after: struct<email:string,first_name:string,id:bigint,last_name:string>, before: string, op: string, source: struct<connector:string,db:string,file:string,gtid:string,name:string,pos:bigint,query:string,row:bigint,server_id:bigint,snapshot:string,table:string,thread:string,ts_ms:bigint,version:string>, transaction: string, ts_ms: bigint, partition: int]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.readStream.format(\"json\").schema(schema).load(OBJECTURL_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62934cf2-878a-4fe7-9b76-011c5b220f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from databricks import koalas as ks\n",
    "kdf = gnames_df.to_koalas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577fcf5f-52d5-4309-8f41-41b21e4a3678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/21 16:27:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(email='sally.thomas@acme.com', first_name='Sally', id=1001, last_name='Thomas')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf['after'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec704d5-09f6-40eb-9def-94660f083489",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dir = \"s3a://test-container/playground/delta-table\"\n",
    "spark.sql(\"CREATE TABLE delta.`%s`(id LONG) USING delta\" % table_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93202326-5d5e-4c7c-bdf3-ef1176583510",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"VACUUM '%s' RETAIN 169 HOURS\" % table_dir).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2571b-acc6-4ab5-be0e-aa86721edeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data = spark.range(8)\n",
    "data = data.withColumn(\"value\", data.id + random.randint(0, 5000))\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1971d-f5db-4097-88a7-eb8fd5d3177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO delta.`%s` VALUES 0, 1, 2, 3, 4\" % table_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ce1a4-1794-451d-bdee-b1c1f30701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM delta.`%s`\" % table_dir).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e81ba-ae6f-4226-b3e5-cc4d050f7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install koalas==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c144e-5f0c-4e84-a702-12a9f29faef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "OBJECTURL_TEST = 's3a://test-container/playground/colors-test' + str(time.time()) + '.csv'\n",
    "rdd = sc.parallelize([('Mario', 'Red'), ('Luigi', 'Green'), ('Princess', 'Pink')])\n",
    "rdd.toDF(['name', 'color']).write.csv(OBJECTURL_TEST, header=True)\n",
    "\n",
    "# Read the data back from MinIO\n",
    "gnames_df = spark.read.format('csv').option('header', True) \\\n",
    "    .load(OBJECTURL_TEST)\n",
    "gnames_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc16fcc-4e9e-43c5-85b7-4c1f90d7c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_URL='s3a://test-container/playground/delts-colors-test' + str(time.time())\n",
    "gnames_df.write.format(\"delta\").save(DELTA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac4e11-027a-4a9f-820a-16fb74d95be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "#data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afaad5-9379-499f-9e1d-9bef4dca173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.format(\"delta\").save(\"s3a://test-container/playground/to_overide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ec175-bf98-4bfd-b0ae-5aa8777a5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://test-container/playground/to_overide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb7f69-501d-4c60-bdc0-f4747827bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import koalas as ks\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea596c-5197-4227-89ba-f4699106a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ks.Series([1, 3, 5, np.nan, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43e847-b267-4e25-a3e1-2b0b69b7d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb304b-41fc-4b5d-be18-fc728e1e67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ddd68-8aaf-44ef-a853-e4ef9318e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "pdf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "kdf = ks.from_pandas(pdf)\n",
    "print(type(kdf))\n",
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfc358-de4a-4ed8-93d1-6142d2ecbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bf484-8341-4c01-b89f-390c7eb0829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = sdf.to_koalas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb0a05-a9b5-4352-a424-3ad2c0c08be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c9e68-c87c-4830-9335-b80ba2987513",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_parquet(\"s3a://test-container/playground/kdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f4818-f300-49b1-afaf-c3de37fa03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.read_parquet(\"s3a://test-container/playground/kdf\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f040a36-7aaf-4861-9992-1f0621546382",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_delta(\"s3a://test-container/playground/delta_partitioned\", mode='overwrite', partition_cols=[\"A\",\"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1563acc-f892-44e7-abc0-4f11c9517bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.read_delta(\"s3a://test-container/playground/delta\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed627b8-51e2-48d3-a3ad-76282b3baff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39da425-79a6-4b7b-9ed9-cdb78f3b9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(8)\n",
    "data = data.withColumn(\"value\", data.id + random.randint(0, 5000))\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://test-container/playground/delta-streaming/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc78bdc-d705-4a23-8143-99987e910770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream writes to the table\n",
    "print(\"####### Streaming write ######\")\n",
    "streamingDf = spark.readStream.format(\"rate\").load()\n",
    "stream = streamingDf.selectExpr(\"value as id\").writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://test-container/playground/delta-streaming/checkpoint\")\\\n",
    "    .start(\"s3a://test-container/playground/delta-streaming/delta-table2\")\n",
    "stream.awaitTermination(10)\n",
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4862c78-d556-434a-b926-797bce8ab028",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM delta.`%s`\" % \"s3a://test-container/playground/delta-streaming/delta-table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2d73b-8074-4ec2-bdc2-6059c03960f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream reads from a table\n",
    "print(\"##### Reading from stream ######\")\n",
    "stream2 = spark.readStream.format(\"delta\").load(\"s3a://test-container/playground/delta-streaming/delta-table2\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "stream2.awaitTermination(10)\n",
    "stream2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71689bbc-453e-44f4-9ad9-e4a8e0fb82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"####### Streaming upgrades in update mode ########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712c105-2dcf-42a9-ae04-6daa80db7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upsert microBatchOutputDF into Delta Lake table using merge\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "    t = deltaTable.alias(\"t\").merge(microBatchOutputDF.alias(\"s\"), \"s.id = t.id\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796e06e-d46d-47d4-8f66-2f032fb67d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "streamingAggregatesDF = spark.readStream.format(\"rate\").load()\\\n",
    "    .withColumn(\"id\", col(\"value\") % 10)\\\n",
    "    .drop(\"timestamp\")\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta Lake table\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://test-container/playground/delta-streaming/delta-table\")\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176b3e9-51fb-4bcd-a70b-dd4702a7dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "stream3.awaitTermination(10)\n",
    "stream3.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155970e-ca3b-41a9-83f9-8cc7979fc842",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingAggregatesDF = spark.readStream.format(\"rate\").load()\\\n",
    "    .withColumn(\"id\", col(\"value\") % 10)\\\n",
    "    .drop(\"timestamp\")\n",
    "# Write the output of a streaming aggregation query into Delta Lake table\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://test-container/playground/delta-streaming/delta-table\")\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()\n",
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "stream3.awaitTermination(10)\n",
    "stream3.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")\n",
    "deltaTable.toDF().show()\n",
    "\n",
    "# Streaming append and concurrent repartition using  data change = false\n",
    "# tbl1 is the sink and tbl2 is the source\n",
    "print(\"############ Streaming appends with concurrent table repartition  ##########\")\n",
    "tbl1 = \"s3a://test-container/playground/delta-streaming/delta-table4\"\n",
    "tbl2 = \"s3a://test-container/playground/delta-streaming/delta-table5\"\n",
    "numRows = 10\n",
    "spark.range(numRows).write.mode(\"overwrite\").format(\"delta\").save(tbl1)\n",
    "spark.read.format(\"delta\").load(tbl1).show()\n",
    "spark.range(numRows, numRows * 10).write.mode(\"overwrite\").format(\"delta\").save(tbl2)\n",
    "\n",
    "\n",
    "# Start reading tbl2 as a stream and do a streaming write to tbl1\n",
    "# Prior to Delta 0.5.0 this would throw StreamingQueryException: Detected a data update in the\n",
    "# source table. This is currently not supported.\n",
    "stream4 = spark.readStream.format(\"delta\").load(tbl2).writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://test-container/playground/delta-streaming/checkpoint/tbl1\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(tbl1)\n",
    "\n",
    "# repartition table while streaming job is running\n",
    "spark.read.format(\"delta\").load(tbl2).repartition(10).write\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"dataChange\", \"false\")\\\n",
    "    .save(tbl2)\n",
    "\n",
    "stream4.awaitTermination(10)\n",
    "stream4.stop()\n",
    "print(\"######### After streaming write #########\")\n",
    "spark.read.format(\"delta\").load(tbl1).show()\n",
    "# cleanup\n",
    "try:\n",
    "    shutil.rmtree(\"s3a://test-container/playground/delta-streaming/\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115d7f5-c138-4cea-9279-8100f3aabd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE delta.`%s`(id LONG)\" % \"s3a://test-container/playground/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a4cfa7c-8598-411a-8f3c-b70b0111a50d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DBUtils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86/2136575097.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mDBUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DBUtils'"
     ]
    }
   ],
   "source": [
    "import DBUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6772144-3e19-4e5d-8f80-784612fe5f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: - ^C\n",
      "failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c gwerbin databricks-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aa4ff7d-476e-4dac-98fa-5bfc3173750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-utils\n",
      "  Downloading databricks-utils-0.0.7.tar.gz (4.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: databricks-utils\n",
      "  Building wheel for databricks-utils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-utils: filename=databricks_utils-0.0.7-py3-none-any.whl size=5251 sha256=a3364bb600cee0c44994bfed0e614778a671e8d7822d196baac9e3854bd48b75\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/98/7e/30/1a3abd6df7bcc291066f0b8be7790406a9b885f6c613e357eb\n",
      "Successfully built databricks-utils\n",
      "Installing collected packages: databricks-utils\n",
      "Successfully installed databricks-utils-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install databricks-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60dbd7d9-374e-44c4-93e9-83a5d7ae23d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86/2364562852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatabricks_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maws\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS3Bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mS3Bucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach_dbutils\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbutils\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from databricks_utils.aws import S3Bucket\n",
    "S3Bucket.attach_dbutils(dbutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d308caec-e75c-44b2-b389-bbf1ef3b31d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`dbutils` not provided. Please call `S3Bucket.attach_dbutils` or provide `dbutils` in the arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86/2142222472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bucket = (S3Bucket(\"somebucketname\", \"SOMEACCESSKEY\", \"SOMESECRETKEY\")\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mallow_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# local spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           .mount(\"somebucketname\"))\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/databricks_utils/aws.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(self, mount_pt, dbutils)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbutils\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             raise RuntimeError(\"`dbutils` not provided. Please call \" +\n\u001b[0m\u001b[1;32m     71\u001b[0m                                \u001b[0;34m\"`S3Bucket.attach_dbutils` or provide \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                \"`dbutils` in the arguments.\")\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `dbutils` not provided. Please call `S3Bucket.attach_dbutils` or provide `dbutils` in the arguments."
     ]
    }
   ],
   "source": [
    "bucket = (S3Bucket(\"somebucketname\", \"SOMEACCESSKEY\", \"SOMESECRETKEY\")\n",
    "          .allow_spark(sc) # local spark context\n",
    "          .mount(\"somebucketname\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df73d9-12d8-43d5-b9f6-b069fe24ea96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
